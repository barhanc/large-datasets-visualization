{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48d3a77a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T11:16:38.339383Z",
     "start_time": "2025-06-09T11:16:36.614611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'link': 'https://www.huffpost.com/entry/covid-boosters-uptake-us_n_632d719ee4b087fae6feaac9', 'headline': 'Over 4 Million Americans Roll Up Sleeves For Omicron-Targeted COVID Boosters', 'category': 'U.S. NEWS', 'short_description': 'Health experts said it is too early to predict whether demand would match up with the 171 million doses of the new boosters the U.S. ordered for the fall.', 'authors': 'Carla K. Johnson, AP', 'date': {'year': 2022, 'month': 9, 'day': 23}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "DATASET_NAME = \"News_Category_Dataset_v3\"\n",
    "\n",
    "with open(f\"{DATA_DIR}/{DATASET_NAME}.json\") as file:\n",
    "    articles = []\n",
    "    for line in file.readlines():\n",
    "        x = json.loads(line.strip(\"\\n\"))\n",
    "        date = x[\"date\"].split(\"-\")\n",
    "        x[\"date\"] = {\"year\": int(date[0]), \"month\": int(date[1]), \"day\": int(date[2])}\n",
    "\n",
    "        articles.append(x)\n",
    "\n",
    "print(articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae987ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T11:17:24.250813Z",
     "start_time": "2025-06-09T11:16:38.392940Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209527/209527 [00:33<00:00, 6171.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_texts(articles: list[dict[str, str]]) -> list[list[str]]:\n",
    "    def _process(text: str) -> list[str]:\n",
    "        return [\n",
    "            LEMMATIZER.lemmatize(token)\n",
    "            for token in word_tokenize(text.lower())\n",
    "            if token.isalpha() and token not in STOPWORDS and len(token) > 2\n",
    "        ]\n",
    "\n",
    "    for article in tqdm(articles):\n",
    "        article[\"processed_short_description\"] = _process(article[\"short_description\"])\n",
    "        article[\"processed_headline\"] = _process(article[\"headline\"])\n",
    "\n",
    "\n",
    "preprocess_texts(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8143f03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T11:17:29.911792Z",
     "start_time": "2025-06-09T11:17:24.256840Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{DATA_DIR}/{DATASET_NAME}_processed.json\", \"w\", encoding=\"utf-8\") as out_f:\n",
    "    json.dump(articles, out_f, ensure_ascii=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1acbc6c3b3fd474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T13:12:07.534951Z",
     "start_time": "2025-06-09T12:46:00.522753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe06e70fe6014d61b211d5baf0fb99f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6548 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "texts = [\" \".join(article[\"processed_headline\"] + article[\"processed_short_description\"]) for article in articles]\n",
    "model = SentenceTransformer(\"distilbert-base-nli-mean-tokens\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "np.save(f\"{DATA_DIR}/{DATASET_NAME}_bert_embeddings.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd32fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
